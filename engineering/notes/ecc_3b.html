<!DOCTYPE html>
<html lang="en">
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" charset="UTF-8">
    <link href='https://fonts.googleapis.com/css?family=Inter' rel='stylesheet'>
    <title>Linear Block Codes II</title>
    <link rel="stylesheet" href="style_notes.css">
    <link rel="icon" type="image/x-icon" href="../../assets/favicon/favicon.ico">
    
    <!-- Include Mathlax CDN -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
            tex2jax: { inlineMath: [ ["$", "$"], ["\\(", "\\)"] ], displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
            TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
            messageStyle: "none"
        });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>

<body>
    <section class="header">
        <div class="title">
            Linear Block Codes II
        </div>
        <div class="author">
            Author: Peter Kim | Professor: Mostafa Darabi
        </div>
    </section>
    <hr><!------------------------------------------------------------------------------------------------------------------------------>

    <!--Start-->
    <p>
        This is a continuation from <a href = "ecc_3a.html">Linear Block Codes I</a>
    </p>

    <h1>Error-Detection Capability</h1>
    <p>
    Error detection is a critical feature of block codes, enabling the receiver to identify distortions introduced during transmission over a noisy channel. 
    </p>

    <h2>Error Detection Mechanism</h2>
    <p>
        When a codeword \( \mathbf{v} \) is transmitted over a binary channel, noise may distort it, resulting in a received vector:
    </p>
    <p>
        \[ \mathbf{r} = \mathbf{v} + \mathbf{e}, \]
    </p>
    <p>
        where \( \mathbf{e} \) is the error pattern, and addition is modulo-2. The Hamming distance between the transmitted and received vectors is:
    </p>
    <p>
        \[ d(\mathbf{v}, \mathbf{r}) = w_H(\mathbf{e}), \]
    </p>
    <p>
        where \( w_H(\mathbf{e}) = \ell \) is the number of errors. For a linear block code \( C(n, k) \) with minimum distance \( d_{\text{min}} \), any two distinct codewords differ by at least \( d_{\text{min}} \) positions. Thus, if \( 0 < \ell < d_{\text{min}} \), \( \mathbf{r} \) cannot be a codeword in \( C \), i.e., \( \mathbf{r} \notin C \).
    </p>
    <p>
        The receiver detects an error if \( \mathbf{r} \) is not a valid codeword, which can be checked using the parity-check matrix \( \mathbf{H} \):
    </p>
    <p>
        \[ \mathbf{r} \cdot \mathbf{H}^T \neq \mathbf{0}. \]
    </p>
    <p>
        The vector \( \mathbf{s} = \mathbf{r} \cdot \mathbf{H}^T \) is called the <em>syndrome</em>. A code with minimum distance \( d_{\text{min}} \) can detect all error patterns with \( d_{\text{min}} - 1 \) or fewer errors. However, if \( \ell = d_{\text{min}} \), \( \mathbf{e} \) could match a codeword, making detection impossible in that case.
    </p>

    <h2>Syndrome Calculation</h2>
    <p>
        The syndrome is computed as:
    </p>
    <p>
        \[ \mathbf{s}_{1 \times (n-k)} = \mathbf{r}_{1 \times n} \cdot \mathbf{H}^T_{n \times (n-k)}, \]
    </p>
    <p>
        where \( \mathbf{s} = (s_0, s_1, \ldots, s_{n-k-1}) \). If \( \mathbf{s} = \mathbf{0} \), \( \mathbf{r} \) is a codeword; otherwise, an error is detected.
    </p>
    <p>
        A linear block code \( C(n, k) \) with minimum distance \( d_{\text{min}} \) can detect \( 2^n - 2^k \) error patterns of length \( n \), as there are \( 2^n \) possible error patterns, and \( 2^k \) are codewords (including the zero vector).
    </p>
    <p>
        And an error pattern \( \mathbf{e} \) is undetectable if it equals a nonzero codeword in \( C \), since then \( \mathbf{r} = \mathbf{v} + \mathbf{e} \) is another codeword, and \( \mathbf{r} \cdot \mathbf{H}^T = \mathbf{0} \).
    </p>

    <h3>Example</h3>
    <p>
        Design a syndrome circuit for the \( C(4, 2) \) code with generator matrix:
    </p>
    <p>
        \[ \mathbf{G} = \begin{bmatrix}
        1 & 0 & 1 & 1 \\
        0 & 1 & 0 & 1
        \end{bmatrix}. \]
    </p>
    <p>
        The parity-check matrix, from \( \mathbf{G} = [\mathbf{I}_2 | \mathbf{P}] \) where \( \mathbf{P} = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} \), is:
    </p>
    <p>
        \[ \mathbf{H} = [\mathbf{P}^T | \mathbf{I}_2] = \begin{bmatrix}
        1 & 0 & 1 & 0 \\
        1 & 1 & 0 & 1
        \end{bmatrix}. \]
    </p>
    <p>
        The syndrome is:
    </p>
    <p>
        \[ \mathbf{s} = (s_0, s_1) = \mathbf{r} \cdot \mathbf{H}^T = (r_0, r_1, r_2, r_3) \cdot \begin{bmatrix}
        1 & 1 \\
        0 & 1 \\
        1 & 0 \\
        0 & 1
        \end{bmatrix} = (r_0 + r_2, r_0 + r_1 + r_3), \]
    </p>
    <p>
        where addition is modulo-2. The syndrome circuit computes:
    </p>
    <p>
        <ul>
            <li>\( s_0 = r_0 + r_2 \) (XOR of \( r_0 \) and \( r_2 \))</li>
            <li>\( s_1 = r_0 + r_1 + r_3 \) (XOR of \( r_0 \), \( r_1 \), and \( r_3 \))</li>
        </ul>
    </p>
    <p>
        If \( \mathbf{s} \neq (0, 0) \), an error is detected.
    </p>
    <figure class="image">
        <img src = "https://i.ibb.co/zVxpJpSV/image.png">
    </figure>

    <h2>Probability of Undetected Errors</h2>
    <p>
        An error pattern is undetectable if it matches a nonzero codeword. For a code \( C(n, k) \), let \( A_i \) be the number of codewords of weight \( i \), forming the weight distribution \( A_0, A_1, \ldots, A_n \). Over a Binary Symmetric Channel (BSC) with error probability \( p \), the probability of an undetected error is:
    </p>
    <p>
        \[ P_U(E) = \sum_{i=1}^n A_i p^i (1 - p)^{n-i}, \]
    </p>
    <p>
        since \( A_0 = 1 \) (the zero codeword) does not contribute to errors. Given \( A_i = 0 \) for \( i < d_{\text{min}} \), this reduces to:
    </p>
    <p>
        \[ P_U(E) = \sum_{i=d_{\text{min}}}^n A_i p^i (1 - p)^{n-i}. \]
    </p>
    <p>
        For the \( C(4, 2) \) code (\( d_{\text{min}} = 2 \)): \( A_0 = 1 \), \( A_1 = 0 \), \( A_2 = 1 \), \( A_3 = 2 \), \( A_4 = 0 \), so:
    </p>
    <p>
        \[ P_U(E) = 1 \cdot p^2 (1 - p)^2 + 2 \cdot p^3 (1 - p)^1 + 0 \cdot p^4 (1 - p)^0 = p^2 (1 - p)^2 + 2 p^3 (1 - p). \]
    </p>

    <h2>Syndrome-Based Error Correction</h2>
    <p>
        The syndrome depends only on the error pattern, not the transmitted codeword. For \( \mathbf{r} = \mathbf{v} + \mathbf{e} \):
    </p>
    <p>
        \[ \mathbf{s} = \mathbf{r} \cdot \mathbf{H}^T = (\mathbf{v} + \mathbf{e}) \cdot \mathbf{H}^T = \mathbf{v} \cdot \mathbf{H}^T + \mathbf{e} \cdot \mathbf{H}^T = \mathbf{0} + \mathbf{e} \cdot \mathbf{H}^T = \mathbf{e} \cdot \mathbf{H}^T, \]
    </p>
    <p>
        since \( \mathbf{v} \in C \) implies \( \mathbf{v} \cdot \mathbf{H}^T = \mathbf{0} \).
    </p>
    <p>
        Error correction proceeds as follows:
    </p>
    <p>
        <ul>
            <li>Compute \( \mathbf{s} = \mathbf{r} \cdot \mathbf{H}^T \).</li>
            <li>If \( \mathbf{s} = \mathbf{0} \), assume \( \mathbf{r} \) is correct (most likely no error).</li>
            <li>If \( \mathbf{s} \neq \mathbf{0} \), find \( \mathbf{e} \) such that \( \mathbf{e} \cdot \mathbf{H}^T = \mathbf{s} \).</li>
        </ul>
    </p>
    <p>
        The equation \( \mathbf{e} \cdot \mathbf{H}^T = \mathbf{s} \) yields \( n - k \) equations with \( n \) unknowns. There are \( 2^k \) solutions (cosets of \( C \)), and the most probable \( \mathbf{e} \) has minimum weight, assuming fewer errors are more likely over a BSC.
    </p>

    <h3>Example</h3>
    <p>
        For a \( C(5, 2) \) code with generator matrix:
    </p>
    <p>
        \[ \mathbf{G} = \begin{bmatrix}
        0 & 1 & 1 & 1 & 0 \\
        1 & 1 & 1 & 0 & 1
        \end{bmatrix}_{2 \times 5}, \]
    </p>
    <p>
        if the observed vector is \( \mathbf{r} = (1, 0, 0, 0, 0) \) over a BSC with \( p = 0.1 \), what is the most probable transmitted codeword?
    </p>
    <p>
        First, compute the parity-check matrix \( \mathbf{H} \). Since \( \mathbf{G} \) is not in systematic form \( [\mathbf{I}_k | \mathbf{P}] \), we derive \( \mathbf{H} \) such that \( \mathbf{G} \cdot \mathbf{H}^T = \mathbf{0} \). Given:
    </p>
    <p>
        \[ \mathbf{H} = \begin{bmatrix}
        1 & 0 & 0 & 0 & 1 \\
        0 & 1 & 0 & 1 & 1 \\
        0 & 0 & 1 & 1 & 1
        \end{bmatrix}_{3 \times 5}, \]
    </p>
    <p>
        we can verify that \( \mathbf{g}_0 \cdot \mathbf{h}_j = 0 \) and \( \mathbf{g}_1 \cdot \mathbf{h}_j = 0 \) for all rows. Compute the syndrome:
    </p>
    <p>
        \[ \mathbf{s} = \mathbf{r} \cdot \mathbf{H}^T = (1, 0, 0, 0, 0) \cdot \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1 \\
        0 & 1 & 1 \\
        1 & 1 & 1
        \end{bmatrix} = (1 + 0 + 0 + 0 + 0, 0 + 0 + 0 + 0 + 0, 0 + 0 + 0 + 0 + 0) = (1, 0, 0). \]
    </p>
    <p>
        Solve for the error pattern \( \mathbf{e} \):
    </p>
    <p>
        \[ (1, 0, 0) = \mathbf{e} \cdot \mathbf{H}^T = (e_0, e_1, e_2, e_3, e_4) \cdot \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1 \\
        0 & 1 & 1 \\
        1 & 1 & 1
        \end{bmatrix}, \]
    </p>
    <p>
        yielding the system:
    </p>
    <p>
        \[
        \begin{cases}
        e_0 + e_4 = 1 \\
        e_1 + e_3 + e_4 = 0 \\
        e_2 + e_3 + e_4 = 0
        \end{cases}
        \]
    </p>
    <p>
        Possible solutions: \( \{(1, 0, 0, 0, 0), (0, 0, 0, 1, 1), (0, 1, 1, 0, 1), (1, 1, 1, 1, 0)\} \) (weights 1, 2, 3, 4). For \( p = 0.1 < 0.5 \), lower weight is more probable, 
        so \( \mathbf{e} = (1, 0, 0, 0, 0) \). Thus:
    </p>
    <p>
        \[ \hat{\mathbf{v}} = \mathbf{r} + \mathbf{e} = (1, 0, 0, 0, 0) + (1, 0, 0, 0, 0) = (0, 0, 0, 0, 0), \]
    </p>
    <p>
        a valid codeword. If we consider \( p = 0.6\), \( p = 0.6 > 0.5 \), higher weight is more probable, so \( \mathbf{e} = (1, 1, 1, 1, 0) \), and:
    </p>
    <p>
        \[ \hat{\mathbf{v}} = (1, 0, 0, 0, 0) + (1, 1, 1, 1, 0) = (0, 1, 1, 1, 0), \]
    </p>
    <p>
        also a codeword.
    </p>

    <h1>Standard Array</h1>
    <p>
        The standard array is a systematic method to partition \( 2^n \) possible received vectors into \( 2^k \) disjoint subsets for decoding a \( C(n, k) \) code.
    </p>

    <h2>Properties of Standard Array</h2>
    <p>
        For a code \( C(n, k) \) with codewords \( \mathbf{v}_0, \mathbf{v}_1, \ldots, \mathbf{v}_{2^k-1} \), the standard array has:
    </p>
    <p>
        <ul>
            <li>(i) Sum of any two vectors in the same row is a codeword.</li>
            <li>(ii) No two vectors in the same row are identical.</li>
            <li>(iii) Every vector appears exactly once.</li>
            <li>(iv) The \( 2^{n-k} \) rows are cosets of \( C \).</li>
            <li>(v) Coset leaders \( \mathbf{e}_i \) head each row.</li>
        </ul>
    </p>
    <p>
        Each column \( D_j = \{\mathbf{v}_j, \mathbf{v}_j + \mathbf{e}_1, \ldots, \mathbf{v}_j + \mathbf{e}_{2^{n-k}-1}\} \) contains \( 2^{n-k} \) vectors, with the top being a codeword.
    </p>

    <h2>Decoding Using Standard Array</h2>
    <p>
        For a received vector \( \mathbf{r} \) in row \( i \) and column \( j \):
    </p>
    <p>
        \[ \mathbf{r} = \mathbf{e}_i + \mathbf{v}_j, \]
    </p>
    <p>
        where \( \mathbf{e}_i \) is the error pattern and \( \mathbf{v}_j \) is the decoded codeword. The \( \mathbf{e}_i \)’s are correctable error patterns.
    </p>

    <h2>Theorems on Error Correction</h2>
    <p>
        <strong>Theorem 1:</strong> A \( C(n, k) \) code can correct \( 2^{n-k} \) error patterns, one per coset.
    </p>
    <p>
        <strong>Theorem 2:</strong> For \( d_{\text{min}} \), all \( n \)-tuples of weight:
    </p>
    <p>
        \[ t = \left\lfloor \frac{d_{\text{min}} - 1}{2} \right\rfloor \]
    </p>
    <p>
        or less can be coset leaders. We can prove this. If \( \mathbf{x} \) and \( \mathbf{y} \) with \( w_H(\mathbf{x}), w_H(\mathbf{y}) \leq t \) are in the same coset, then \( \mathbf{x} + \mathbf{y} \in C \) and:
    </p>
    <p>
        \[ w_H(\mathbf{x} + \mathbf{y}) \leq w_H(\mathbf{x}) + w_H(\mathbf{y}) \leq 2t < d_{\text{min}}, \]
    </p>
    <p>
        a contradiction unless \( \mathbf{x} = \mathbf{y} \). Thus, all \( \sum_{i=0}^t \binom{n}{i} \leq 2^{n-k} \) patterns fit.
    </p>
    <p>
        <strong>Theorem 3:</strong> All vectors in a coset have the same syndrome: \( (\mathbf{e}_i + \mathbf{v}_j) \cdot \mathbf{H}^T = \mathbf{e}_i \cdot \mathbf{H}^T \).
    </p>
    <p>
        <strong>Theorem 4:</strong> Different cosets have distinct syndromes: if \( \mathbf{e}_i \cdot \mathbf{H}^T = \mathbf{e}_j \cdot \mathbf{H}^T \), then \( \mathbf{e}_i + \mathbf{e}_j \in C \), implying \( i = j \) for distinct coset leaders.
    </p>

    <h3>Example</h3>
    <p>
        Construct the standard array for the \( C(5, 2) \) code with:
    </p>
    <p>
        \[ \mathbf{G} = \begin{bmatrix}
        0 & 1 & 1 & 1 & 0 \\
        1 & 1 & 1 & 0 & 1
        \end{bmatrix}, \]
    </p>
    <p>
        minimizing error probability for \( p < 0.5 \). Codewords:
    </p>
    <p>
        \[
        \begin{aligned}
        \mathbf{v}_0 &= (0, 0) \cdot \mathbf{G} = (0, 0, 0, 0, 0), \\
        \mathbf{v}_1 &= (0, 1) \cdot \mathbf{G} = (1, 1, 1, 0, 1), \\
        \mathbf{v}_2 &= (1, 0) \cdot \mathbf{G} = (0, 1, 1, 1, 0), \\
        \mathbf{v}_3 &= (1, 1) \cdot \mathbf{G} = (1, 0, 0, 1, 1).
        \end{aligned}
        \]
    </p>
    <p>
        Standard array (coset leaders of minimum weight):
    </p>
    <p>
        <table style = "width: 100%">
            <tr><td>(0, 0, 0, 0, 0)</td><td>(1, 1, 1, 0, 1)</td><td>(0, 1, 1, 1, 0)</td><td>(1, 0, 0, 1, 1)</td></tr>
            <tr><td>\( \mathbf{e}_1 = (1, 0, 0, 0, 0) \)</td><td>(0, 1, 1, 0, 1)</td><td>(1, 1, 1, 1, 0)</td><td>(0, 0, 0, 1, 1)</td></tr>
            <tr><td>\( \mathbf{e}_2 = (0, 1, 0, 0, 0) \)</td><td>(1, 0, 1, 0, 1)</td><td>(0, 0, 1, 1, 0)</td><td>(1, 1, 0, 1, 1)</td></tr>
            <tr><td>\( \mathbf{e}_3 = (0, 0, 1, 0, 0) \)</td><td>(1, 1, 0, 0, 1)</td><td>(0, 1, 0, 1, 0)</td><td>(1, 0, 1, 1, 1)</td></tr>
            <tr><td>\( \mathbf{e}_4 = (0, 0, 0, 1, 0) \)</td><td>(1, 1, 1, 1, 1)</td><td>(0, 1, 1, 0, 0)</td><td>(1, 0, 0, 0, 1)</td></tr>
            <tr><td>\( \mathbf{e}_5 = (0, 0, 0, 0, 1) \)</td><td>(1, 1, 1, 0, 0)</td><td>(0, 1, 1, 1, 1)</td><td>(1, 0, 0, 1, 0)</td></tr>
            <tr><td>\( \mathbf{e}_6 = (1, 1, 0, 0, 0) \)</td><td>(0, 0, 1, 0, 1)</td><td>(1, 0, 1, 1, 0)</td><td>(0, 1, 0, 1, 1)</td></tr>
            <tr><td>\( \mathbf{e}_7 = (1, 0, 1, 0, 0) \)</td><td>(0, 1, 0, 0, 1)</td><td>(1, 1, 0, 1, 0)</td><td>(0, 0, 1, 1, 1)</td></tr>
        </table>
    </p>
    <p>
        Coset leaders are chosen with minimum weight (mostly weight 1) for \( p < 0.5 \), ensuring maximum likelihood decoding (MLD).
    </p>

    <h1>Look-Up Table Decoding</h1>
    <p>
        Storing the entire standard array is inefficient. Instead, a look-up table maps syndromes to correctable error patterns.
    </p>

    <h2>Syndrome Look-Up Table</h2>
    <p>
        For \( C(n, k) \), there are \( 2^{n-k} \) syndromes, each corresponding to a unique coset leader. Steps:
    </p>
    <p>
        <ol>
            <li>Compute \( \mathbf{s} = \mathbf{r} \cdot \mathbf{H}^T \).</li>
            <li>Find \( \mathbf{e}_\ell \) such that \( \mathbf{e}_\ell \cdot \mathbf{H}^T = \mathbf{s} \).</li>
            <li>Decode \( \hat{\mathbf{v}} = \mathbf{r} + \mathbf{e}_\ell \).</li>
        </ol>
    </p>

    <h3>Example</h3>
    <p>
        For the \( C(5, 2) \) code, with \( \mathbf{H} = \begin{bmatrix} 1 & 0 & 0 & 0 & 1 \\ 0 & 1 & 0 & 1 & 1 \\ 0 & 0 & 1 & 1 & 1 \end{bmatrix} \), and \( \mathbf{r} = (0, 0, 1, 1, 0) \) over a BSC with \( p < 0.5 \):
    </p>
    <p>
        Syndrome table:
    </p>
    <p>
        <table>
            <tr><th>Error Pattern (Coset Leader)</th><th>Syndrome</th></tr>
            <tr><td>(0, 0, 0, 0, 0)</td><td>(0, 0, 0)</td></tr>
            <tr><td>(1, 0, 0, 0, 0)</td><td>(1, 0, 0)</td></tr>
            <tr><td>(0, 1, 0, 0, 0)</td><td>(0, 1, 0)</td></tr>
            <tr><td>(0, 0, 1, 0, 0)</td><td>(0, 0, 1)</td></tr>
            <tr><td>(0, 0, 0, 1, 0)</td><td>(0, 1, 1)</td></tr>
            <tr><td>(0, 0, 0, 0, 1)</td><td>(1, 1, 1)</td></tr>
            <tr><td>(1, 1, 0, 0, 0)</td><td>(1, 1, 0)</td></tr>
            <tr><td>(1, 0, 1, 0, 0)</td><td>(1, 0, 1)</td></tr>
        </table>
    </p>
    <p>
        Compute:
    </p>
    <p>
        \[ \mathbf{s} = (0, 0, 1, 1, 0) \cdot \mathbf{H}^T = (0 + 0 + 1 + 1 + 0, 0 + 0 + 0 + 1 + 0, 0 + 0 + 1 + 1 + 0) = (0, 1, 0), \]
    </p>
    <p>
        so \( \mathbf{e} = (0, 1, 0, 0, 0) \), and:
    </p>
    <p>
        \[ \hat{\mathbf{v}} = (0, 0, 1, 1, 0) + (0, 1, 0, 0, 0) = (0, 1, 1, 1, 0), \]
    </p>
    <p>
        a codeword, consistent with MLD for \( p < 0.5 \).
    </p>

    <h2>Logic Level Implementation</h2>
    <p>
        Look-up table decoding can be implemented efficiently using switching functions derived from the syndrome table. For a linear block code \( C(n, k) \), the decoding table serves as a truth table for \( n \) functions:
    </p>
    <p>
        \[ e_i = f_i(s_0, s_1, \ldots, s_{n-k-1}), \quad i = 0, 1, \ldots, n-1, \]
    </p>
    <p>
        where \( s_i \) is the \( i \)-th entry of the syndrome vector \( \mathbf{s} \), and \( e_i \) is the \( i \)-th entry of the estimated error pattern \( \mathbf{e} \). These functions can be expressed using logical operations (e.g., AND, OR, NOT) based on the syndrome-to-error mapping.
    </p>
    <p>
        For example, in the \( C(5, 2) \) code from Example (***), the syndrome table showed:
    </p>
    <p>
        <ul>
            <li>\( \mathbf{e} = (0, 0, 0, 0, 1) \) with \( \mathbf{s} = (1, 1, 1) \), so \( e_4 = s_0 \land s_1 \land s_2 \),</li>
            <li>\( \mathbf{e} = (1, 0, 0, 0, 0) \) with \( \mathbf{s} = (1, 0, 0) \), so \( e_0 = s_0 \land \neg s_1 \land \neg s_2 \).</li>
        </ul>
    </p>
    <p>
        These logical expressions can be implemented in hardware (e.g., using gates) for real-time decoding, avoiding the need to store the entire standard array.
    </p>

    <h1>Error-Detection and Error-Correction Capabilities</h1>
    <p>
        The error-detection and correction capabilities of a block code are closely tied to its weight distribution and that of its dual code. This section revisits error detection and introduces polynomial representations and the MacWilliams identity.
    </p>

    <h2>Weight Distribution in Polynomial Form</h2>
    <p>
        The weight distribution of \( C(n, k) \) can be represented as a polynomial:
    </p>
    <p>
        \[ A(z) = A_0 + A_1 z + A_2 z^2 + \cdots + A_n z^n, \]
    </p>
    <p>
        where \( A_0 = 1 \) and \( A_i \) counts codewords of weight \( i \). Similarly, for the dual code \( C_d(n, n-k) \), the weight distribution is:
    </p>
    <p>
        \[ B(z) = B_0 + B_1 z + B_2 z^2 + \cdots + B_n z^n, \]
    </p>
    <p>
        with \( B_0 = 1 \). The MacWilliams identity relates these polynomials:
    </p>
    <p>
        \[ A(z) = 2^{n-k} (1 + z)^n B\left( \frac{1 - z}{1 + z} \right). \]
    </p>
    <p>
        This identity allows computation of \( A(z) \) from \( B(z) \) or vice versa, leveraging the duality between \( C \) and \( C_d \).
    </p>

    <h2>Probability of Undetected Error via Dual Code</h2>
    <p>
        Using the MacWilliams identity, the probability of an undetected error can be expressed in terms of the dual code’s weight distribution. For \( C(n, k) \) over a BSC with error probability \( p \):
    </p>
    <p>
        \[ P_U(E) = 2^{n-k} B(1 - 2p) - (1 - p)^n, \]
    </p>
    <p>
        where:
    </p>
    <p>
        \[ B(1 - 2p) = \sum_{i=0}^n B_i (1 - 2p)^i. \]
    </p>
    <p>
        This follows because \( P_U(E) \) is the probability of receiving a nonzero codeword in \( C \), and the dual code’s weight enumerator evaluated at \( 1 - 2p \) accounts for the channel’s effect, adjusted by subtracting the probability of no error (\( (1 - p)^n \)).
    </p>

    <hr><!------------------------------------------------------------------------------------------------------------------------------>
    <section class="footer">
        <h3>Notes</h3>
        <p>
            [a]
        </p>
        <h3>References</h3>
        <p>
            [1]
        </p>
    </section>
</body>
</html>
