<!DOCTYPE html>
<html lang="en">
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" charset="UTF-8">
    <link href='https://fonts.googleapis.com/css?family=Inter' rel='stylesheet'>
    <title>Weight Initialization</title>
    <link rel="stylesheet" href="../../assets/style.css">
    <link rel="icon" type="image/x-icon" href="../../assets/favicon/favicon.ico">
    
    <!-- Include MathJax CDN -->
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
</head>

<body>
    <section class="header">
        <div class="header-col-L2">Weight Initialization</div>
        <div class="header-col-R"></div>
    </section>

    <section>
        How should we initialize the neural network? Well, there are a lot of criterions for good initialization exist, e.g., be close to some local optima, have a higher chance to reach a global optima.
        But most of them are not easily computable before training the neural networks. One computable criterion is that we want to start with some stable initial
        neural network.
        <br><br>

        <h1>Math Review</h1>
        Let us recap some basic facts about variance:
        \[
            V[X] = \mathbb{E} \left[(X - \mathbb{E}[X])^2\right] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2
        \]

        For two independent random variables \( X \) and \( Y \), we have:
        \begin{align}
            V[XY] &= \mathbb{E}[X^2 Y^2] - (\mathbb{E}[XY])^2 \\
            &= \mathbb{E}[X^2 Y^2] - \mathbb{E}[X]^2 \mathbb{E}[Y]^2 \\
            &= \mathbb{E}[X^2] \mathbb{E}[Y^2] - \mathbb{E}[X]^2 \mathbb{E}[Y^2] + \mathbb{E}[X]^2 \mathbb{E}[Y^2] - \mathbb{E}[X]^2 \mathbb{E}[Y]^2 \\
            &= (\mathbb{E}[X^2] - \mathbb{E}[X]^2) \mathbb{E}[Y^2] + \mathbb{E}[X]^2 (\mathbb{E}[Y^2] - \mathbb{E}[Y]^2) \\
            &= V[X] \mathbb{E}[Y^2] + \mathbb{E}[X]^2 V[Y] \\
            &= V[X] (V[Y] + \mathbb{E}[Y]^2) + \mathbb{E}[X]^2 V[Y] \\
            &= V[X]V[Y] + \mathbb{E}[Y]^2 V[X] + \mathbb{E}[X]^2 V[Y]
        \end{align}    
        If \( \mathbb{E}[X] = \mathbb{E}[Y] = 0 \), then:
        \[
            V[XY] = V[X] V[Y]
        \]
        Additionally:
        \[
            \mathbb{E}[XY] = \mathbb{E}[X] \mathbb{E}[Y]
        \]
        \[
            \mathbb{E}[X^2 Y^2] = \mathbb{E}[X^2] \mathbb{E}[Y^2]
        \]
        <br><br>

        <!------------------------------------------------------------------------------------------------------------------------------>
        <h1>Forward Analysis</h1>
        Recall:
        \[
            h_2 = \sigma (W_2 h_1)
        \]
        If we assume Tanh activation, \( \sigma(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \) and we are in the linear regime:
        <figure class="image" style="width:70%; max-width:100%;" data-ckbox-resource-id="cDsFgyNwdFVs">
            <picture>
                <source srcset="https://ckbox.cloud/a97183991c8954c75f0d/assets/cDsFgyNwdFVs/images/80.webp 80w,https://ckbox.cloud/a97183991c8954c75f0d/assets/cDsFgyNwdFVs/images/160.webp 160w,https://ckbox.cloud/a97183991c8954c75f0d/assets/cDsFgyNwdFVs/images/240.webp 240w,https://ckbox.cloud/a97183991c8954c75f0d/assets/cDsFgyNwdFVs/images/320.webp 320w,https://ckbox.cloud/a97183991c8954c75f0d/assets/cDsFgyNwdFVs/images/400.webp 400w,https://ckbox.cloud/a97183991c8954c75f0d/assets/cDsFgyNwdFVs/images/480.webp 480w,https://ckbox.cloud/a97183991c8954c75f0d/assets/cDsFgyNwdFVs/images/560.webp 560w,https://ckbox.cloud/a97183991c8954c75f0d/assets/cDsFgyNwdFVs/images/616.webp 616w" sizes="(max-width: 616px) 100vw, 616px" type="image/webp">
                <img src="https://ckbox.cloud/a97183991c8954c75f0d/assets/cDsFgyNwdFVs/images/616.png" width="616" height="316" style="max-width:100%; height:auto;">
            </picture>
        </figure>
        Then we have:
        \[
            h_2[i] = \sigma ( \sum_j^{N_i} W_2[i,j] h_1[j] ) \approx \sum_{j=1}^{N_1}W_2[i,j]h_1[j]
        \]
        We further assume i.i.d and zero mean with same variance for all activations \(h_1[j]\) and all weights \(W_2[i,j]\) at all layers, then:
        \[
            V[h_2[i]] = V[\sum_{j=1}^{N_1}W_2[i,j]h_1[j]] = N_1 V[W_2[i,j]]V[h_1[j]]
        \]
        This relation holds for all layers given the assumptions. When we unroll the recursion, we have:
        \[
            V[h_l[i]]] \approx V[x[j]] \prod_{k=1}^{l}N_{k-1}V[W_k[i,j]] \quad \text{[a]}
        \]
        To preserve the variance of activations through forward pass, \( V[h_l[i]] \approx V[x[j]] \), we can simply set:
        \[
            V[W_k[i,j]] = \frac{1}{N_{k-1}}
        \]
        To verify this, let's look at the graident w.r.t activations:
        \[
            \frac{\partial L}{\partial h_1[j]} = \sum_{i=1}^{N_2} \frac{\partial L}{\partial h_2[i]} \frac{\partial h_2[i]}{\partial h_1[j]} \approx \sum_{i=1}^{N_2} \frac{\partial L}{\partial h_2[i]} W_2[i,j]
        \]
        Let us again assume i.i.d. and zero mean with same variance for all 
        \(\frac{\partial L}{\partial h_2[i]}\), \(W_2[i,j]\), and \(h_2[i]\) at all layers, then:
        \begin{align}
            V \left[ \frac{\partial L}{\partial h_1[j]} \right] &\approx \sum_{i=1}^{N_2} V \left[ \frac{\partial L}{\partial h_2[i]} \right] V \left[ W_2[i,j] \right] \\
            &= N_2 V \left[ \frac{\partial L}{\partial h_2[i]} \right] V \left[ W_2[i,j] \right] \\
            &= V \left[ \frac{\partial L}{\partial y[i]} \right] \prod_{k=2}^{M+1} N_k V \left[ W_k[i,j] \right]
        \end{align}
        Setting \( V [W_k[i,j]] = \frac{1}{N_k} \) preserves the variance of gradients w.r.t. activations. What about the gradients w.r.t. weights?
        \[
            \frac{\partial L}{\partial W_2[i,j]} = \frac{\partial L}{\partial h_2[i]} \frac{\partial h_2[i]}{\partial W_2[i,j]} \approx \frac{\partial L}{\partial h_2[i]} h_1[j]
        \]        
        \begin{align}
            V \left[ \frac{\partial L}{\partial W_2[i,j]} \right] &\approx V \left[ \frac{\partial L}{\partial h_2[i]} \right] V \left[ h_1[j] \right] \\
            &= \left( V \left[ \frac{\partial L}{\partial y[i]} \right] \prod_{k=3}^{M+1} N_k V \left[ W_k[i,j] \right] \right) \left( V [x[j]] \prod_{k=1}^{1} N_{k-1} V [W_k[i,j]] \right) \\
            &= \frac{N_0}{N_1} \left( \prod_{k=1}^{1} N_k V [W_k[i,j]] \right) \left( \prod_{k=3}^{M+1} N_k V [W_k[i,j]] \right) V \left[ \frac{\partial L}{\partial y[i]} \right] V [x[j]] \\
            &= \frac{N_0}{N_1} V \left[ \frac{\partial L}{\partial y[i]} \right] V [x[j]]
        \end{align}
        Setting \( V [W_k[i,j]] = \frac{1}{N_k} \) makes the variance of gradients w.r.t. weights behave reasonably (e.g., no exploding or vanishing).
        In summary, to preserve the variance of activations, we set
        \[
            V [W_k[i,j]] = \frac{1}{N_{k-1}}
        \]
        to preserve the variance of gradients w.r.t. activations, we set
        \[
            V [W_k[i,j]] = \frac{1}{N_k}
        \]
        To compromise between two goals, we can take the mean of the denominators:
        \[
            V [W_k[i,j]] = \frac{1}{\frac{N_k + N_{k-1}}{2}} = \frac{2}{N_k + N_{k-1}}
        \]
        This is the so-called “Xavier Initialization”. By considering the effect of ReLU, we can similarly derive “Kaiming Initialization”.
        <br><br>

        <!------------------------------------------------------------------------------------------------------------------------------>
        <i>
            <b>Notes</b><br>
            [a] Note that i and j here are arbitrary since we assume all activations have the same variance and all weights have the same variance as well.
            <br><br>

            <b>References</b><br>
            [1] https://lrjconan.github.io/UBC-CPEN455-DL/
        </i>
        <br><br>
    </section>
</body>
</html>
