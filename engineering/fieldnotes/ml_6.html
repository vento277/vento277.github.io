<!DOCTYPE html>
<html lang="en">
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" charset="UTF-8">
    <link href='https://fonts.googleapis.com/css?family=Inter' rel='stylesheet'>
    <title>Support Vector Regression</title>
    <link rel="stylesheet" href="../../assets/style.css">
    <link rel="icon" type="image/x-icon" href="../../assets/favicon/favicon.ico">
    
    <!-- Include Mathlax CDN -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
            tex2jax: { inlineMath: [ ["$", "$"], ["\\(", "\\)"] ], displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
            TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
            messageStyle: "none"
        });
    </script>

    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

</head>

<body>
    <section class="header">
        <div class="header-col-L2">Support Vector Regression</div>
        <div class="header-col-R"></div>
    </section>

    <section>
        <!------------------------------------------------------------------------------------------------------------------------------>
        Support Vector Regression (SVR) is a powerful machine learning technique that extends the principles of Support Vector Machines (SVMs) to regression problems. Given a training dataset \( D = \{ (x_1, y_1), (x_2, y_2), \dots, (x_n, y_n) \} \), where \( x_i \in \mathbb{R} \) represents the input features and \( y_i \in \mathbb{R} \) the target values, we aim to find a function that predicts \( y \) based on \( x \). The regression model in SVR is typically linear and can be written as:
        \[
            f(x) = \langle w, x \rangle + b
        \]
        Unlike traditional linear regression, where the goal is to minimize the squared error between the predicted \( f(x) \) and the actual \( y \), SVR introduces a tolerance for small errors. Specifically, SVR allows an error margin \( \epsilon \) around the true value \( y \), within which no penalty is incurred. This makes SVR robust to minor deviations and noise in the data.
        <br><br>

        The objective of SVR is to find \( w \) and \( b \) such that the predictions \( f(x) \) are as close as possible to the true values \( y \), while keeping the model simple (i.e., minimizing the complexity of \( w \)). The optimization problem can be initially expressed as:
        \[
            \min_{w, b} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n l_\epsilon(y_i - f(x_i))
        \]
        where \( l_\epsilon \) is the \( \epsilon \)-insensitive loss function, defined as:
        \[
            L_\epsilon(z) = 
            \begin{cases} 
            0 & \text{if } |z| \leq \epsilon \\
            |z| - \epsilon & \text{if } |z| > \epsilon
            \end{cases}
        \]
        This loss function ignores errors smaller than \( \epsilon \) and penalizes only those exceeding it linearly.

        To handle cases where data points fall outside this \( \epsilon \)-tube, we introduce slack variables \( \xi_i \) and \( \xi_i^* \) for each training point \( i \), representing deviations above and below the tube, respectively. The optimization problem becomes:
        \[
            \min_{w, b, \xi_i, \xi_i^*} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n (\xi_i + \xi_i^*)
        \]
        subject to:
        \[
            y_i - (w^\top x_i + b) \leq \epsilon + \xi_i, \quad (w^\top x_i + b) - y_i \leq \epsilon + \xi_i^*, \quad \xi_i, \xi_i^* \geq 0, \quad \forall i
        \]

        To solve this constrained optimization problem, we use Lagrange multipliers (\( \alpha_i, \alpha_i^*, \eta_i, \eta_i^* \)) and form the Lagrangian. Taking partial derivatives with respect to \( w \), \( b \), \( \xi_i \), and \( \xi_i^* \) and setting them to zero, we obtain:<br>
        - \( w = \sum_{i=1}^n (\alpha_i - \alpha_i^*) x_i \)<br>
        - \( \sum_{i=1}^n (\alpha_i - \alpha_i^*) = 0 \)<br>
        - \( C = \alpha_i + \eta_i \) and \( C = \alpha_i^* + \eta_i^* \), with \( \eta_i, \eta_i^* \geq 0 \)
        <br><br>

        Substituting these into the Lagrangian, we derive the dual problem:
        \[
            \max_{\alpha, \alpha^*} -\frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n (\alpha_i - \alpha_i^*)(\alpha_j - \alpha_j^*) x_i^\top x_j - \epsilon \sum_{i=1}^n (\alpha_i + \alpha_i^*) + \sum_{i=1}^n y_i (\alpha_i - \alpha_i^*),
        \]
        subject to:
        \[
            \sum_{i=1}^n (\alpha_i - \alpha_i^*) = 0, \quad 0 \leq \alpha_i, \alpha_i^* \leq C.
        \]

        Therfore, the solution to SVR takes the form:
        \[
            f(x) = \sum_{i=1}^n (\alpha_i - \alpha_i^*) x_i^\top x + b,
        \]
        where only the points with \( \alpha_i - \alpha_i^* \neq 0 \) contribute to the model. These points lie on or outside the \( \epsilon \)-tube and are called \emph{support vectors}. This sparsity is a key feature of SVR, making it computationally efficient.
        <br><br>

        <hr>
        <i>
            <b>Notes</b><br>
            [a] \( \epsilon \)-Insensitive Loss is unique to SVR and reflects its robustness to small errors, unlike squared loss in linear regression.
            <br><br>
            [b] Support Vectors is only a subset of training points (support vectors) define the SVR model, making it memory-efficient.
            <br><br>
            [c] Both SVR optimization and tree splitting use greedy strategies, prioritizing local optima that lead to global solutions.
            <br><br>
            <b>References</b><br>
            [1]  
        </i>

        <br><br>
    </section>
</body>
</html>
