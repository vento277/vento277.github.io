<!DOCTYPE html>
<html lang="en">
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" charset="UTF-8">
    <link href='https://fonts.googleapis.com/css?family=Inter' rel='stylesheet'>
    <title>Linear Models</title>
    <link rel="stylesheet" href="style.css">
    <link rel="icon" type="image/x-icon" href="../../assets/favicon/favicon.ico">
    
     <!-- Include Mathlax CDN -->
     <script type="text/x-mathjax-config">
        MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
            tex2jax: { inlineMath: [ ["$", "$"], ["\\(", "\\)"] ], displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
            TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
            messageStyle: "none"
        });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>

<body>
    <section class="header">
        <div class="header-col-L2">Linear Models</div>
        <div class="header-col-R"></div>
    </section>

    <section>
    <h1>Setup</h1>
    Training data are sampled from an unknown data distribution in an independent and indetically distributed fashion:
    \[
        (x_n, y_n) \sim P_{\text{data}}(x, y) \quad n = 1, \ldots, N
    \]

    Therefore, the training dataset:
    \[
        D = \{ (x_n, y_n) | n = 1, \ldots, N \} \sim P_{\text{data}}(x,y)^N
    \]

    Here, both input and output can be continuous or discrete scalars, vectors, tensors, sets, sequences, graphs...etc:
    \[
        \text{regression}: x_n \in \mathbb{R}^2 \quad y_n \in \mathbb{R} \quad
        \text{classification}: x_n \in \mathbb{R}^2 \quad y_n \in {1,2,\ldots,K}
    \]

    Let's introduce a model \( f(x, w) \) with learnable parameters \( w \). This model belong to a hypothesis class \( H \)[a].
    We denote all linear models with weight norm no larger than 1 as:
    \[
        H = \{ f(x,w) | f(x,w) = w^Tx, ||w|| <= 1 \}
    \]

    Loss is denoted as:
    \[
        l = (y, f(x,w))
    \]

    And the errors are denoted as:
    \[
        \text{Generalization Error}: \mathbb{E_{\mathbb{P}_{data(x,y)}}}[(y,f(x,w))] \quad
        \text{Training Error}: \dfrac{1}{N} \sum_{n=1}^{N}l(y_n,f(x_n,w))
    \]

    Ideally, we want to find a model n the hypothesis class that minimizes the generalization error. But since
    risk is incomputable[b] so we can approximate it via. traning error. This learning framework is called empirical risk minimization.
    <br><br>

    <h1>Linear Regression</h1>
    Given the setup above, we define a linear model[c] as:
    \[
        \hat{y} = w^Tx + b \quad \text{where b is bias term}
    \]
    
    Therefore, the loss is:
    \[
        L(\{ \hat{y}_n \}, \{ y_n \}) = \dfrac{1}{N} \sum_{n-1}^N l(\hat{y_n}, y_n)
    \]
    \[
        \text{MSE (L2)}: l(\hat{y_n}, y_n) = || \hat{y}_n - y_n ||_2^2 \quad 
        \text{MAE (L1)}: l(\hat{y_n}, y_n) = || \hat{y}_n - y_n ||_1 
    \]
    \[
        \text{Smooth L1}(x) = \begin{cases}
        \dfrac{1}{2\beta} || \hat{y}_n - y_n ||_2^2 & \text{if} || \hat{y}_n - y_n ||_1 < 1 \\
        || \hat{y}_n - y_n ||_1 - \dfrac{1}{2\beta} & \text{otherwise} 
        \end{cases}
    \]
    The \( \beta \) for smooth L1 acts as a scaling factor that controls the transition between the L2 loss and L1 loss.
    It controls the "smoothness" of the transition between L2 and L1. Smaller values of \( \beta \) make the transition occur more quickly (favoring L1 loss), 
    while larger values of \( \beta \) result in a more gradual transition.
    <br><br>

    <h2>Learning Algorithm</h2>
    Learning is an optimization problem. So the algorithm is just an optimization algorithm. Recall MSE:
    \[
        L(w, b) = \dfrac{1}{N} \sum_{n=1}^N || \hat{y}_n - y_n ||_2^2
    \]

    Substituting:
    \[
        \dfrac{1}{N} \sum_{n=1}^N || (w^T x_n + b) - y_n ||_2^2
    \]

    To simplify the gradient calculation, we can write the equation above as:
    \[
        L(w, b) = \dfrac{1}{2N} \sum_{n=1}^N || (w^T x_n + b) - y_n ||_2^2
    \]

    One of the most efficient way to minimize a loss function (or cost function) and optimize the parameters of a model
    is to use the gradient descent. To find the gradient of the loss function with respect to the weight vector \( w \), we need to compute:
    \[
        \frac{\partial L(w, b)}{\partial w}
    \]  

    Differentiating the squared term \( \left( (w^T x_n + b) - y_n \right)^2 \) with respect to \( w \):
    \[
    \frac{\partial}{\partial w} \left( (w^T x_n + b) - y_n \right)^2 = 2 \left( (w^T x_n + b) - y_n \right) \cdot x_n
    \]
    
    Thus:
    \[
    \frac{\partial \mathcal{L}(w, b)}{\partial w} = \frac{1}{2N} \sum_{n=1}^N 2 \left( (w^T x_n + b) - y_n \right) x_n
    \]

    Simplifying:
    \[
    \frac{\partial \mathcal{L}(w, b)}{\partial w} = \frac{1}{N} \sum_{n=1}^N \left( (w^T x_n + b) - y_n \right) x_n
    \]

    Similarly, the gradient for \( b \) is:
    \[
    \frac{\partial \mathcal{L}(w, b)}{\partial w} = \frac{1}{N} \sum_{n=1}^N \left( (w^T x_n + b) - y_n \right)
    \]
    <br>

    
    <!------------------------------------------------------------------------------------------------------------------------------>
    <i>
        <b>Notes</b><br>
        [a] Hypothesis class consists of all possible hypotheses that you are searching over, regardless of their form
        <br><br>
        [b] The risk is incomputable beacuse:<br>
        - Lack of knowledge of the true data distribution.<br>
        - The high complexity of deep learning models.<br>
        - The intractability of computing expectations over the true data distribution.<br>
        - Overfitting issues that prevent reliable computation of the generalization error.<br>
        - The reliance on empirical approximations like training and test error, which can never exactly capture the true risk.
        <br><br>
        [c]
        The term inference is used in many contexts. In DL and ML, it 
        ypically means the computational process from input to output, e.g., the forward pass of a feedforward neural network.
        In graphical models, 
        it typically means computing the marginal probability or the maximum a posterior (MAP) estimation.
        And in statistics, it typically means estimating the parameters of the model, which is called learning/training in DL/ML.
        <br><br>

        Our inference is just our linear model. 
        For other models in DL/ML e.g., deep energy based models, one may need both of first two.
        <br><br>
        <b>References</b><br>
        [1]  https://lrjconan.github.io/UBC-CPEN455-DL/
    </i>

    <br><br>
       
    </section>
</body>
</html>
