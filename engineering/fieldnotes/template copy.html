<!DOCTYPE html>
<html lang="en">
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" charset="UTF-8">
    <link href='https://fonts.googleapis.com/css?family=Inter' rel='stylesheet'>
    <title>Template</title>
    <link rel="stylesheet" href="style.css">
    <link rel="icon" type="image/x-icon" href="../../assets/favicon/favicon.ico">
    
    <!-- Include Mathlax CDN -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
            tex2jax: { inlineMath: [ ["$", "$"], ["\\(", "\\)"] ], displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
            TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
            messageStyle: "none"
        });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>

<body>
    <section class="header">
        <div class="title">
            Template
        </div>
        <div class="author">
            Author: Peter Kim | Professor: Name
        </div>
    </section>
    <hr><!------------------------------------------------------------------------------------------------------------------------------>

    <!--Start-->
    <p>
        Recurrent Neural Networks (RNNs) are a type of artificial neural network designed to process sequential data, 
        such as time series, text, speech, or any data where the order of elements matters. 
        Unlike traditional feedforward neural networks, RNNs have connections that form directed cycles, 
        allowing them to maintain a "memory" of previous inputs and use this information to influence future outputs.
    </p>
    <p>
        RNNs (Recurrent Neural Networks) marked the beginning of sequential modeling, which later evolved into more advanced architectures like 
        Long Short-Term Memory (LSTM) and Transformers. Understanding RNNs is essential for building a strong foundation to grasp these later models. 
        Despite their limitations, RNNs have unique advantages. They are well-suited for time-series data, such as speech recognition and language processing, 
        and they benefit from parameter efficiency due to shared weights across time steps.
    </p>
        
    <h1>RNN Cell Unit</h1> 
    <p> 
        It processes sequential data by maintaining a hidden state that captures information from previous time steps. 
        At each time step, the cell takes an input, combines it with the hidden state from the previous step, 
        and produces an output along with an updated hidden state. 
    </p>
    <figure class="image">
        <source><img style="max-width:100%; height:auto;" src="https://i.ibb.co/q3z6NR28/image.png">
    </figure>
    <p>
        The RNN Cell Unit operates using the following steps: 
        <ul> 
            <li>
                Input:Takes an input vector \( x_t \) at time step \( t \).
            </li> 
            <li>
                Hidden State Update: Combines the input \( x_t \) with the previous hidden state \( h_{t-1} \) using weights and biases.
            </li> 
            <li>
                Activation: Applies an activation function (e.g., tanh) to produce the new hidden state \( h_t \).
            </li> 
            <li>
                Output: Generates an output \( y_t \) based on the updated hidden state \( h_t \).
            </li> 
        </ul> 
    </p>
    <p> 
        The operations of an RNN Cell Unit can be expressed as \( h_t = f_W (h_{t-1}, x_t)\). 
        And we can process a sequence of vectors $\mathbb{x}$ by applying a recurrence
        formula at every time step:
    </p>
    <p>
        \[
        Output: y_t = f_{W_{hy}}(h_t),
        \]
    </p>
    <p>
        where $f_W$ and $f_{W_{hy}}$ is a function with parameters $W$. 
    </p> 
    <p>
        Therefore, the vanilla formula of RNN (typical activation function of tanh) can be expressed as:
    </p>
    <p>
        \[ h_t = tanh(W_{hh} h_{t-1} + W_{hx} x_t) \],
    </p>
    <p>
        where \( W_{hh}, W_{hx} \) are weight matrices.
    </p>

    <h2>Computational Graph</h2>
    <p>
        RNNs can be thought of as multiple copies of the same network, each
        passing a message to a successor. 
    </p>
    <figure class="image">
        <source><img style="max-width:100%; height:auto;" src="https://i.ibb.co/bjYTj606/image.png">
    </figure>
    <a class="caption">The same function and the same set of parameters are used at every
        time step.
    </a>

    <p>
        Below is a computations graph of a RNN:
    </p>
    <figure class="image">
        <source><img style="max-width:100%; height:auto;" src="https://i.ibb.co/mCXczTmB/image.png">
    </figure>
    <a class="caption">RNN Computational Graph with Shared Weights</a>
    <p>
        Note that:
        <ul>
            <li>$(h_1,y_1) = F(h_0,x_1,W)$</li>
            <li>$(h_2,y_2) = F(h_1,x_2,W)$</li>
            <li>$(x_1,x_2)$ compress a length-2 sequence</li>
        <ul>
    </p>

    <figure class="image">
        <source><img style="max-width:100%; height:auto;" src="https://i.ibb.co/mCXczTmB/image.png">
    </figure>
    <a class="caption">RNN Computational Graph with Shared Weights</a>

    
    <hr><!------------------------------------------------------------------------------------------------------------------------------>
    <section class="footer">
        <h3>Notes</h3>
        <p>
            [a]
        </p>
        <h3>References</h3>
        <p>
            [1]
        </p>
    </section>
</body>
</html>
