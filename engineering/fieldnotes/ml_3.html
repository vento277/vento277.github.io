<!DOCTYPE html>
<html lang="en">
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" charset="UTF-8">
    <link href='https://fonts.googleapis.com/css?family=Inter' rel='stylesheet'>
    <title>Model Evaluation and Training</title>
    <link rel="stylesheet" href="../../assets/style.css">
    <link rel="icon" type="image/x-icon" href="../../assets/favicon/favicon.ico">
    
    <!-- Include Mathlax CDN -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
            tex2jax: { inlineMath: [ ["$", "$"], ["\\(", "\\)"] ], displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
            TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
            messageStyle: "none"
        });
    </script>

    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

</head>

<body>
    <section class="header">
        <div class="header-col-L2">Model Evaluation and Training</div>
        <div class="header-col-R"></div>
    </section>

    <section>
        <!------------------------------------------------------------------------------------------------------------------------------>
        <h1>Logistic Regression for Classification</h1>
        Logistic regression is essential for understanding how to model binary outcomes, which is a foundational concept in classification tasks. It builds on the idea of linear regression by introducing a probabilistic framework, which is crucial for many real-world applications like medical diagnosis, spam detection, and more. 
        <br><br>
        
        <h2>Model Definition</h2>
        The logistic function is introduced to map linear combinations of inputs to probabilities, ensuring outputs are within the (0, 1) range.
        <br><br>

        The predicted probability is given by the logistic function:
        \[
        P(\hat{y} = 1 \mid X) = \text{logit}^{-1}(X^T \Theta ) =\frac{1}{1 + e^{-X^T \Theta}} \in (0, 1)
        \]
        where \(X\) is the input feature vector, and \(\Theta\) is the learnable parameter vector (similar to weights in linear regression).
        <br><br>
        
        <h2>Classification Rule</h2>
        The classification rule demonstrates how probabilities are converted into class labels.
        To make a decision:<br>
        - If $P(\hat{y} = 1 \mid X) \geq 0.5$, predict $\hat{y} = 1$.<br>
        - Otherwise, predict $\hat{y} = 0$.
        <br><br>

        This threshold of 0.5 corresponds to the point where the odds are balanced.
        <br><br>

        <h1>Supervision Criteria</h1>
        The supervision criteria explain how the model is trained to align predicted probabilities with true labels.

        For a training sample $({x}^{(i)}, y^{(i)})$, where $y^{(i)} \in \{0, 1\}$ is the true label, we define the predicted probability:
        \[
        P(\hat{y}^{(i)} = 1 \mid X^{(i)}) = \sigma (X^{(i)^T} \Theta) = \frac{1}{1 + e^{-X^{(i)^T} {\Theta}}}.
        \]

        Our goal here is to adjust ${\Theta}$ so that:<br>
        - If $y^{(i)} = 1$, then $P(\hat{y}^{(i)} = 1 \mid X^{(i)})$ is as close to 1 as possible.<br>
        - If $y^{(i)} = 0$, then $P(\hat{y}^{(i)} = 1 \mid X^{(i)})$ is as close to 0 as possible (equivalently, $1 - P(\hat{y}^{(i)} = 1 \mid X^{(i)})$ is close to 1).
        <br><br>

        <h2>Probability of Correct Prediction</h2>
        This connects the predicted probabilities to the likelihood of correct predictions.
        Define the probability that the prediction matches the true label:
        \[
        P_i = \left( P(\hat{y}^{(i)} = 1 \mid X^{(i)}) \right)^{y^{(i)}} \left( 1 - P(\hat{y}^{(i)} = 1 \mid X^{(i)}) \right)^{1 - y^{(i)}}.
        \]
        This captures both cases:<br>
        - $y^{(i)} = 0$, then $P_i = P(\hat{y}^{(i)} = 0 \mid X^{(i)})$.<br>
        - $y^{(i)} = 1$, then $P_i = P(\hat{y}^{(i)} = 1 \mid X^{(i)})$.<br>
        <br><br>

        <h1>Cross-Entropy Error Function</h1>
        The cross-entropy error function quantifies the discrepancy between predicted probabilities and true labels, providing a clear objective for optimization during training.
        We use the binary cross-entropy loss, derived from information theory:
        \[
        \text{Error} = - y \ln(P(y)) - (1 - y) \ln(1 - P(y)), \quad y \in \{0, 1\},
        \]
        where $P(y) = P(\hat{y}^{(i)} = 1 \mid X^{(i)})$.

        Here, we can interpret the equation as:<br>
        - If $y = 1$, the error simplifies to $-\ln(P(y))$, which grows large if $P(y)$ is far from 1.<br>
        - If $y = 0$, the error becomes $-\ln(1 - P(y))$, which grows large if $P(y)$ is far from 0.
        <br><br>

        And the total loss over $n$ samples is:
        \[
        L = -\frac{1}{n} \sum_{i=1}^n \left[ y^{(i)} \ln(P(y)^{(i)}) + (1 - y^{(i)}) \ln(1 - P(y)^{(i)}) \right]
        \]
        <br><br>

        <h1>Extension to Multiple Classes</h1>
        Extending logistic regression to multiple classes generalizes the binary case, enabling the model to handle more complex classification problems.
        For $K \geq 2$ classes, we use multinomial logistic regression (softmax regression).
        For class $k$ (where $k = 1, 2, \dots, K-1$):
        \[
        \ln \left( \frac{P(Y = k \mid X)}{P(Y = 0 \mid X)} \right) = X^T {\Theta}_k
        \]

        And the probabilities are:
        \[
        P(Y = k \mid X) = \frac{e^{X^T {\Theta}_k}}{1 + \sum_{l=1}^{K-1} e^{X^T {\Theta}_l}}, \quad k = 1, 2, \dots, K-1,
        \]
        \[
        P(y = 0 \mid X) = \frac{1}{1 + \sum_{l=1}^{K-1} e^{X^T {\Theta}_l}}
        \]
        <br>
        
        <h2>Loss Function</h2>
        We want to maximize the likelihood of the data, which is equivalent
        to minimizing the log-likelihood:
        \[
        L = - \sum_{i=1}^n ln(P(Y^{(i)} = y_i \mid X^{(i)})) = \sum_{i=1}^n ln(1 + e^{\Theta_1^T X^{(i)}} + e^{\Theta_2^T X^{(i)}}) - {\Theta_{y_i}^T} X^{(i)}
        \]
        where $\Theta_0$ is all zeros by definition.  
        <br><br>

        <h1>Evaluating Classification Models</h1>
        How do we know if our model is good? We use performance metrics based on predictions and ground truth.
        <br><br>

        <h2>Confusion Matrix Basics</h2>
        The confusion matrix provides a detailed breakdown of model performance, helping to identify specific strengths and weaknesses.
        It is a table showing TP, FP, TN, and FN for each class. 
        <br><br>
        For binary classification:<br>
        - True Positives (TP): Correctly predicted as class 1.<br>
        - False Positives (FP): Incorrectly predicted as class 1.<br>
        - True Negatives (TN): Correctly predicted as class 0.<br>
        - False Negatives (FN): Incorrectly predicted as class 0.
        <br><br>
        For multi-class problems, it becomes a $K \times K$ matrix, with rows as actual classes and columns as predicted classes.
        <figure class="image" style="width:70%; max-width:100%;" data-ckbox-resource-id="ytPLzmTB2cLe">
            <picture>
                <source srcset="https://ckbox.cloud/2ce57516f4580910f6af/assets/ytPLzmTB2cLe/images/80.webp 80w,https://ckbox.cloud/2ce57516f4580910f6af/assets/ytPLzmTB2cLe/images/160.webp 160w,https://ckbox.cloud/2ce57516f4580910f6af/assets/ytPLzmTB2cLe/images/240.webp 240w,https://ckbox.cloud/2ce57516f4580910f6af/assets/ytPLzmTB2cLe/images/320.webp 320w,https://ckbox.cloud/2ce57516f4580910f6af/assets/ytPLzmTB2cLe/images/400.webp 400w,https://ckbox.cloud/2ce57516f4580910f6af/assets/ytPLzmTB2cLe/images/480.webp 480w,https://ckbox.cloud/2ce57516f4580910f6af/assets/ytPLzmTB2cLe/images/560.webp 560w,https://ckbox.cloud/2ce57516f4580910f6af/assets/ytPLzmTB2cLe/images/568.webp 568w" sizes="(max-width: 568px) 100vw, 568px" type="image/webp"><img src="https://ckbox.cloud/2ce57516f4580910f6af/assets/ytPLzmTB2cLe/images/568.png" style="max-width:100%; height:auto;" width="568" height="411">
            </picture>
        </figure>
        <br>

        <h2>Metrics</h2>
        Many performance metrics are defined based on these:<br>
        - Accuracy: Fraction of correct predictions:
        \[
        \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
        \]

        - Precision: Accuracy of positive predictions:
        \[
        \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
        \]

        - Recall(Sensitivity): Accuracy for the positive class:
        \[
        \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
        \]

        - F1 score: The harmonic mean of precision and recall. It balances precision and recall, with a maximum value of 1:
        \[
        F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
        \]

        - Receiver Operating Characteristic (ROC) curve: Plots the True Positive Rate (Recall) against the False Positive Rate ($\frac{\text{FP}}{\text{FP} + \text{TN}}$) as the classification threshold varies. It visualizes the trade-off between sensitivity and specificity:
        <figure class="image image_resized" style="width:70%; max-width:100%;" data-ckbox-resource-id="rL6GJp4BrYdI">
            <picture>
                <source srcset="https://ckbox.cloud/2ce57516f4580910f6af/assets/rL6GJp4BrYdI/images/94.webp 94w,https://ckbox.cloud/2ce57516f4580910f6af/assets/rL6GJp4BrYdI/images/188.webp 188w,https://ckbox.cloud/2ce57516f4580910f6af/assets/rL6GJp4BrYdI/images/282.webp 282w,https://ckbox.cloud/2ce57516f4580910f6af/assets/rL6GJp4BrYdI/images/376.webp 376w,https://ckbox.cloud/2ce57516f4580910f6af/assets/rL6GJp4BrYdI/images/470.webp 470w,https://ckbox.cloud/2ce57516f4580910f6af/assets/rL6GJp4BrYdI/images/564.webp 564w,https://ckbox.cloud/2ce57516f4580910f6af/assets/rL6GJp4BrYdI/images/658.webp 658w,https://ckbox.cloud/2ce57516f4580910f6af/assets/rL6GJp4BrYdI/images/752.webp 752w,https://ckbox.cloud/2ce57516f4580910f6af/assets/rL6GJp4BrYdI/images/846.webp 846w,https://ckbox.cloud/2ce57516f4580910f6af/assets/rL6GJp4BrYdI/images/931.webp 931w" sizes="(max-width: 931px) 100vw, 931px" type="image/webp"><img src="https://ckbox.cloud/2ce57516f4580910f6af/assets/rL6GJp4BrYdI/images/931.png"  style="max-width:100%; height:auto;" width="931" height="924">
            </picture>
        </figure>
        <br>

        <hr>
        <i>
            <b>Notes</b><br>
            [a] 
            <br><br>
            [b] 
            <br><br>
            [c]
            <br><br>
            <b>References</b><br>
            [1]  
        </i>
        <br><br>
    </section>
</body>
</html>
