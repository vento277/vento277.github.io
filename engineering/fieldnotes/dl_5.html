<!DOCTYPE html>
<html lang="en">
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" charset="UTF-8">
    <link href='https://fonts.googleapis.com/css?family=Inter' rel='stylesheet'>
    <title>Learning Rate, Momentum and Adam</title>
    <link rel="stylesheet" href="style.css">
    <link rel="icon" type="image/x-icon" href="../../assets/favicon/favicon.ico">
    
    <!-- Include MathJax CDN -->
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
</head>

<body>
    <section class="header">
        <div class="header-col-L2">Learning Rate, Momentum and Adam</div>
        <div class="header-col-R"></div>
    </section>

    <section>
        Previously, we have discussed the method of computing graidents. Let us see how we perofmrn gradient-based learning, such as, Stochastic Gradient Descent (SGD).
        <br><br>
        
        <!------------------------------------------------------------------------------------------------------------------------------>
        <h1>Stochastic Gradient Descent</h1>
        Stochastic Gradient Descent (SGD) is an optimization algorithm used to minimize a loss function.
        Itâ€™s a variant of the traditional gradient descent algorithm, with the key difference being that it updates the parameters using a small, random subset (a mini-batch) of the data at each step, rather than using the entire dataset.
        <br><br>
        Because it only uses one or few data points, each update tends to be faster. Also, It allows us to optimize on large datasets where using the full dataset would be too computationally expensive. But one of its disadvantages is nvatiagting ravines.
        <br><br>
        Ravines refer to areas in a loss landscape (the "surface" we're trying to optimize) where the curvature of the surface is significantly different in different directions. This happens when one direction is much steeper (higher gradient) than another.
        Imagine a 2D hill with a steep drop in one direction (e.g., left to right) and a shallow drop in the other direction (e.g., forward and backward). The steep direction would be like a cliff, and the shallow direction would feel more like a long, gentle slope:
        <figure class="image" style="width:70%; max-width:100%;"  data-ckbox-resource-id="1cuoIoRQeFdi">
            <picture>
                <source srcset="https://ckbox.cloud/a97183991c8954c75f0d/assets/1cuoIoRQeFdi/images/80.webp 80w,https://ckbox.cloud/a97183991c8954c75f0d/assets/1cuoIoRQeFdi/images/160.webp 160w,https://ckbox.cloud/a97183991c8954c75f0d/assets/1cuoIoRQeFdi/images/240.webp 240w,https://ckbox.cloud/a97183991c8954c75f0d/assets/1cuoIoRQeFdi/images/320.webp 320w,https://ckbox.cloud/a97183991c8954c75f0d/assets/1cuoIoRQeFdi/images/400.webp 400w,https://ckbox.cloud/a97183991c8954c75f0d/assets/1cuoIoRQeFdi/images/480.webp 480w,https://ckbox.cloud/a97183991c8954c75f0d/assets/1cuoIoRQeFdi/images/560.webp 560w,https://ckbox.cloud/a97183991c8954c75f0d/assets/1cuoIoRQeFdi/images/632.webp 632w" sizes="(max-width: 632px) 100vw, 632px" type="image/webp">
                <img src="https://ckbox.cloud/a97183991c8954c75f0d/assets/1cuoIoRQeFdi/images/632.png" width="632" height="530" style="max-width:100%; height:auto;">>
            </picture>
        </figure>
        In these regions, SGD may struggle to make efficient progress because it can get stuck or make inconsistent updates. SGD updates parameters based on the gradient, but in ravines, the gradient can point in very different directions, making the optimization process less stable and slow.
        <br><br>

        <h2>Momentum</h2>
        Gradient descent is a man walking down a hill. He follows the steepest path downwards; his progress is slow, but steady.
        Momentum is a heavy ball rolling down the same hill. The added inertia acts both as a smoother and an accelerator, dampening
        oscillations and causing us to barrel through narrow valleys, small humps and local minima.
        <br><br>
        
        Now, the algorithm for SGD with momentum is given as:<br>
        - Input: Step \( T \), learning rate \( \eta \), initial weights \( W^0 \), batch size \( B \), momentum \( \beta \), initial momentum \( M^0 \).
        For \( t = 1, \dots, T \):<br>
        -- Get mini-batch data \( (X^t, Y^t) \)<br>
        -- Compute the loss function:
            \[
            L(W^{t-1}, X^t, Y^t) = \frac{1}{B} \sum_{i=1}^{B} l(f(W^{t-1}, X^t[i]), Y^t[i])
            \]<br>
        -- Compute the momentum term:
            \[
            M^t = \beta M^{t-1} + \frac{\partial L (W^{t-1}, X^t, Y^T)}{\partial W}
            \]<br>
        -- Update the weights:
            \[
            W^t = W^{t-1} - \eta M^t
            \] <br>
        - Return \(W^T \)
        <br><br>

        Denoting \(g^t = \frac{\partial L (W^{t-1}, X^t, Y^t)}{\partial W} \), recall that we have the following update rule in the case of \( \beta = 0 \):
        \[
            W^t = W^{t-1} - \eta g^t
        \]
        How can we tune the learning rate? AdaGrad (Adaptive Gradient) proposes to assign larger learning rates to infrequently updated weights and smaller ones to
        frequently updated weights:
        \[
            G^t = \sum_{\tau = 1}^t g^{\tau} g^{\tau T} \quad \text{In practive, we only need} \quad g^t \odot g^t
        \]
        \[
            W^t = W^{t-1} - \eta diag(\epsilon I + G^t)^{-1/2} g^t
        \]
        The element-wise update rule of AdaGrad is:
        \[
            W^t[i] = W^{t-1}[i] - \frac{\eta}{\sqrt{\epsilon + G^t[i,i]^2}}g^t[i]
        \]
        Since we accumulate (squared) gradients from the beginning, our learning rate is always decaying and could vanish before we
        converge. To deal with this, RMSProp [6] proposes to use exponential moving average (EMA) of past squared gradients:
        \[
            G^t = pG^{t-1} + (1-p)g^t {g^{t}}^{T}
        \]
        \[
            W^t = W^{t-1} - \eta diag(\epsilon I + G^t)^{-1/2} g^t
        \]
        Now that we can tune the learning rate adaptively, how can we incorporate momentum? Based on RMSProp, Adam (Adaptive Momentum) proposes to keep another EMA of past gradients:
        \[
            g^{t} = \beta_1 g^{t - 1} + (1-\beta_1)g^T
        \]
        \[
            G^t = \beta_2 G^{t-1} + (1-\beta_2)g^t {g^t}^T \quad \text{In practive, we only need} \quad g^t \odot g^t
        \]
        \[
            W^t = W^{t-1} - \eta_t diag(\epsilon I + G^t)^{-1/2} g^{t}
        \]
        \[
            \eta_t = \eta \frac{\sqrt{1 - \beta^t_2}}{1 - \beta^t_1} \quad \text{Where} \quad \frac{\sqrt{1 - \beta^t_2}}{1 - \beta^t_1} \text{is the correction term}
        \]
        The correction term comes from computing the exptected value:
        \[
            \mathbb{E} [g^{t}] = \mathbb{E} [\beta_1 g^{\sim t - 1} + (1-\beta_1)g^T] = (1 - \beta_1^t) \mathbb{E} [g^i]
        \]
        Similar with \(G^t\):
        \[
            \mathbb{E} [G^{t}] \approx (1 - \beta_2^t) \mathbb{E} [g^i {g^i}^T]
        \]
        Now to check the updates in \(\eta diag(\epsilon I + G^t)^{-1/2} g^t\):
        \[
            \mathbb{E}[\Delta W^t[j]] \approx \eta_t \frac{(1-\beta^t_1 \mathbb{E}[g^i[j]])}{\sqrt{1-\beta^t_2} \sqrt{\mathbb{E}[(g^i[j])^2]}} \leq \eta_t \frac{1 - \beta^t_1}{\sqrt{1 - \beta^t_2}} = \eta
        \]
        When \(\beta_1 = 0\) almost the same as RMSProp besides the correction term:
        \[
            G^t = p \beta_2 G^{t-1} + (1-p)g^t {g^t}^T \quad 
        \]
        \[
            W^t = W^{t-1} - \eta_t diag(\epsilon I + G^t)^{-1/2} g^{t}
        \]
        In summary the algorithm for Adam is given as:<br>
        - Input: Step \( T \), learning rate \( \eta \), initial weights \( W^0 \), batch size \( B \), momentum \( \beta_1 = 0.9 \quad \beta_2 = 0.999 \), \( \epsilon = 10^{-8} \), \( m^0 = 0 \), \(v^0 = 0 \).
        For \( t = 1, \dots, T \):<br>
        -- Get mini-batch data \( (X^t, Y^t) \)<br>
        -- Compute the loss function:
            \[
            L(W^{t-1}, X^t, Y^t) = \frac{1}{B} \sum_{i=1}^{B} l(f(W^{t-1}, X^t[i]), Y^t[i])
            \]<br>
        -- Compute the momentum term:
            \[
                g^t = \frac{\partial L(W^{t-1}, X^t, Y^t)}{\partial W}
            \]
            \[
                m^t = \beta_1 m^{t-1} + (1-\beta_1)g^t
            \]
            \[
                v^t = \beta_2 v^{t-1} + (1-\beta_2)(g^t \odot g^t)
            \]
            <br>
        -- Update the weights:
            \[
            W^t = W^{t-1} - \eta \frac{\sqrt{1-\beta^t_2}}{1-\beta^t_1} \frac{m^t}{\sqrt{v^t} + \epsilon}
            \] <br>
        - Return \(W^T \)
        <br><br>

        <!------------------------------------------------------------------------------------------------------------------------------>
        <h1>Weight Decay</h1>
        Suppose we observe overfitting and want to regularize the complexity of our neural networks to reduce it. We can penalize the weights
        so that they are not far from 0, thus restricting the set of models we are considering:
        \[
            L(W, X, Y) = \frac{1}{B} \sum_{i=1}^B l(f(W,X[i]), Y[i])
        \]
        \[
            \hat{L}(W, X, Y) = L(W, X, Y) + \frac{\lambda}{2} ||\text{Vec}(W)||^2_2
        \]
        We can control the lambda to trade-off model complexity and overfitting. It only adds a term to the existing gradient:
        \[
            \frac{\partial{\hat{Y}}}{\partial W} = \frac{\partial L}{\partial W} + \lambda W
        \]
        For methods like SGD, SGD + Momentum, we directly add this term to the gradient and then perform gradient update.
        But for Adam, we have two possibilities: 1. add it to the gradient and then perform gradient update; 2. add it to the gradient update
        This ordering change makes a difference in practice and 2 is called AdamW.
        <br><br>

        Going back to Adam, we can place the \(g^t\):
        \[
            g^t = \frac{\partial L(W^{t-1}, X^t, Y^t)}{\partial W} + \lambda W
        \]
        Which makes weight decay also goes through EMA.
        Also for \(W^t\):
        \[
            W^t = W^{t-1} - \eta (\frac{\sqrt{1-\beta^t_2}}{1-\beta^t_1} \frac{m^t}{\sqrt{v^t} + \epsilon} + \lambda W^{t-1})
        \]
        <br>

        <h2>Early Stops</h2>
        We do not necessarily need to run the optimization until the maximum number of iterations or until its convergence.
        We can check the validation error and if it is keep increasing within a time window, then we stop the training.
        If you worry that it will go down again, run until the maximum number of iterations and pick the model with the least validation error.
        <br><br>

        <!------------------------------------------------------------------------------------------------------------------------------>
        <i>
            <b>Notes</b><br>
            <br><br>

            <b>References</b><br>
            [1] https://lrjconan.github.io/UBC-CPEN455-DL/ 
        </i>

        <br><br>
    </section>
</body>
</html>
