<!DOCTYPE html>
<html lang="en">
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" charset="UTF-8">
    <link href='https://fonts.googleapis.com/css?family=Inter' rel='stylesheet'>
    <title>Finite Fields</title>
    <link rel="stylesheet" href="style_notes.css">
    <link rel="icon" type="image/x-icon" href="../assets/favicon/favicon.ico">
    
    <!-- Include Mathlax CDN -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
            tex2jax: { inlineMath: [ ["$", "$"], ["\\(", "\\)"] ], displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
            TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
            messageStyle: "none"
        });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>

<body>
    <section class="header">
        <div class="title">
            K-Nearest Neighbor Classification
        </div>
        <div class="author">
            Author: Peter Kim | Professor: Xiaoxiao Li
        </div>
    </section>
    <hr><!------------------------------------------------------------------------------------------------------------------------------>

    <!--Start-->
    <p>
        K-Nearest Neighbors (KNN) is a machine learning algorithm that predicts what category something belongs to by looking at the categories of the things closest to it. 
        It looks at "K" number of similar (nearest) data points and picks the most similar one.
        Once it does that, it takes a vote to find the category that appears most in the neighbour. 
    </p>
    <p>
        Imagine you want to know what kind of fruit a new fruit is.
        KNN would look at the fruits closest to it (in terms of color, size, shape, etc.).
        If most of the closest fruits are apples, it will guess that the new fruit is also an apple.
    </p>
    <p>
        As one might have already predicted, KNN does both classification and regression. 
        And beacuse it dosen't require any past data, the model does not build from the training data. 
        The classification time is linear in training set size for each test cases
    </p>
        
    <h1>Intuition</h1>
    <p>
        To classify a test instance d, define K-neighborhood P as k nearest neighbors of d.
        The predicted label for d is the majority class in P.
        If K=1, then we'll use only the nearest neighbor to determine the
        class of a data point. If the value of K=10, then we'll use the 10 nearest neighbors, and so on.
    </p>
        
    <h1>Algorithm</h1>
    <p>
        For each data point in the testing data:
        <ol>
            <li>
                Find the distance to all training data samples.
                The method in which this is done depends on the application. 
                (Euclidean distance, Manhattan distance, 
                Hamming distance, Minkowski distance, etc.)
            </li>
            <li>Store the distances on an ordered list and sort them</li>
            <li>Choose the top K entries from the sorted list</li>
            <li>
                Label the test point based on the majority of classes present in 
                the selected points
            </li>
        </ol>
    </p>

    <h2>Similar Data</h2>
    <p>
        Distance between two points shows us how two data points are similar to one another.
        As previously mentioned, there are many ways of going about this.
    </p>

    <p>
        First is the Euclidean distance (L2 norm), $dist(a,b) = \sqrt{\sum_{i=1}^n (a_i - b_i)^2}$.
        Second is the Manhattan distance (L1 norm), $dist(a,b) = \sum_{i=1}^n |a_i - b_i|$. 
        Third is the Chebyshev distance, $dist(a,b) = max_i(|a_i - b_i|)$
    </p>

    <h1>Remarks</h1>
    <p>
        KNN can deal with complex and arbitrary decision boundaries.
        Despite its simplicity, researchers have shown that the classification
        accuracy of KNN can be quite strong and in many cases as
        accurate as those elaborated methods. However, KNN is slow at the classification time.
    </p> 
    <p>
        Also, as one's intuition might suggest, the increase of K leads to increase in bias and decrease in variance
        as the model becomes smoother and less sensitive to indivisual data points. 
    </p> 
    <hr><!------------------------------------------------------------------------------------------------------------------------------>
    <section class="footer">
        <h3>Notes</h3>
        <p>
            [a]
        </p>
        <h3>References</h3>
        <p>
            [1]
        </p>
    </section>
</body>
</html>
